{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5ee67d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    derivate_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "    x = range(-10, 10)\n",
    "    plt.title(\"Actual Derivatives vs. Estimate\")\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')            # red\n",
    "    plt.plot(x, list(map(derivate_estimate, x)), 'b+', label='Estimate')     # blue\n",
    "    plt.legend(loc=9)\n",
    "    plt.show()\n",
    "    \n",
    "    #pick a random starting point\n",
    "    v = [random.randint(-10,10) for i in range(3)]\n",
    "    \n",
    "    tolerance = 0.0000001\n",
    "    \n",
    "    while True:\n",
    "        gradient = sum_of_squares_gradient(v)\n",
    "        next_v = step(v, gradient, -0.01)\n",
    "        if distance_v1(next_v, v) < tolerance:\n",
    "            break\n",
    "        v = next_v\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i \n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    '''subtracts corresponding elements'''\n",
    "    result = [v_i - w_i for v_i, w_i in zip(v, w)]    # [1, 2] and [2, 1] results [1-2, 2-1]\n",
    "    return result\n",
    "\n",
    "def dot(v, w):\n",
    "    '''v_1 * w_1 + ... + v_n * w_n'''\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def square_distance(v, w):\n",
    "    '''(v_1 - w1)^2 + ... + (v_n - w_n)^2'''\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance_v1(v, w):\n",
    "    return math.sqrt(square_distance(v, w))\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"return a new function that's the same as f,\n",
    "    except that it outputs inifinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    '''use gradient descent to find theta that minimize target function'''\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_size]\n",
    "        \n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "            \n",
    "def negate(f):\n",
    "    \"return a function that for any input x return -f(x)\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "        \n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data = zip(x, y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta, min_value = None, float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement = 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "    \n",
    "    return min_theta\n",
    "            \n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    \n",
    "    #pick a random starting point\n",
    "    v = [random.randint(-10,10) for i in range(3)]\n",
    "    \n",
    "    tolerance = 0.0000001\n",
    "    \n",
    "    while True:\n",
    "        gradient = sum_of_squares_gradient(v)\n",
    "        next_v = step(v, gradient, -0.01)\n",
    "        if distance_v1(next_v, v) < tolerance:\n",
    "            break\n",
    "        v = next_v\n",
    "        \n",
    "    step_size = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
